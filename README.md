ğŸ’¼ AbhiSearch â€“ AI Resume Assistant
--------------------------------------
ğŸ” Overview
---------------
AbhiSearch is an AI-powered resume analysis tool that enables users to:
-------------------------------------------------------------------------

ğŸ“„ Upload their resume in PDF format

â“ Ask any question related to their resume (e.g., â€œWhat are my strengths?â€ or â€œHow can I improve it?â€)

ğŸ“Š Or ask the AI to rate their resume and provide improvement suggestions

This project uses the LLaMA 3 model via Ollama for local AI-powered analysis.

âš™ï¸ Tech Stack
------------------------
Layer	Technology
Frontend	ğŸ§© Streamlit
Backend	ğŸ› ï¸ Django REST Framework
AI Model	ğŸ§  LLaMA 3 via Ollama
PDF Parsing	ğŸ“„ PyMuPDF (fitz)

ğŸ“ Project Structure
------------------------------------------------------------------------
bash
Copy
Edit
abhigpt-search/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ views.py        # Core backend logic (upload, process, respond)
â”‚   â””â”€â”€ urls.py         # API routing
â”‚
â”œâ”€â”€ app.py              # Streamlit frontend UI
â”œâ”€â”€ venv/               # Python virtual environment
â”œâ”€â”€ README.md           # Project documentation
â””â”€â”€ docs/               # (Optional) Test PDF uploads


ğŸ”„ How It Works
------------------
ğŸ§¾ User uploads a PDF resume through the Streamlit interface.

ğŸ—£ï¸ User either:

Asks a specific question (e.g., â€œSummarize my strengthsâ€)

Or requests a resume rating (e.g., â€œRate my resumeâ€)

ğŸ› ï¸ The Django backend
-----------------------

Extracts text from the PDF using fitz (PyMuPDF)

Constructs a prompt based on the resume content and user query

ğŸ§  The prompt is passed to Ollama, running the LLaMA 3 model locally.

ğŸ’¬ The AI generates a response which is returned to the frontend.

ğŸ“² The response is displayed to the user in Streamlit.

ğŸ”§ Backend: API Highlights
------------------------------

from rest_framework.decorators import api_view
from rest_framework.response import Response
from django.http import JsonResponse
import fitz  # PyMuPDF for PDF text extraction
import subprocess  # To call local Ollama model

ğŸ“¥ Upload & Analyze Endpoint
-----------------------------------

@api_view(["POST"])
def upload_pdf(request):
    pdf_file = request.FILES.get("file")  # User-uploaded resume (PDF)
    query = request.POST.get("query", "")  # User's input query

    # Step 1: Read PDF using fitz
    doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    text = "".join([page.get_text() for page in doc])

    # Step 2: Create AI prompt
    if "rate my resume" in query.lower() or "score" in query.lower():
        prompt = f"... rate this resume and provide suggestions ..."
    else:
        prompt = f"... answer this query based on the resume ..."

    # Step 3: Run LLaMA 3 via Ollama
    result = subprocess.run(["ollama", "run", "llama3"], input=prompt, ...)

    # Step 4: Return AI response
    return Response({"answer": result.stdout.strip()})

    -----------------------------------------------------------------------------------------------------
ğŸ§ª Example Queries
âœ… â€œWhat are the strong points in my resume?â€

âœ… â€œGive me suggestions to improve this resumeâ€

âœ… â€œRate my resumeâ€

âœ… Requirements
Python 3.10+

<!-- Streamlit -->

Django REST Framework

PyMuPDF (pip install PyMuPDF)

Ollama with LLaMA 3 model installed locally (ollama run llama3)


# 1. Clone the repo

# 2. Create virtual environment
python -m venv venv
venv\Scripts\activate    # On Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Start backend (Django API)
cd backend
python manage.py runserver


